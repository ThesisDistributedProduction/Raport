% !TeX spellcheck = en_GB
%\clearpage
\section{Resource management} % Load balancing
\label{cha:resourceManagement}

%Below should
%Having redundant systems provides a number of favourable traits like scalability and availability.
%\paragraph{The setting}
The current Siemens system communicates with multiple external systems which enable external configuration of control parameters or external monitoring of power production. The protocols used for the external communication are HTTP, Modbus and other protocols specific to wind farms. Currently the system is designed to handle 100 concurrent external communication connections.
%The system also currently has a central database responsible for doing backup of all data produced by the turbines as well as calculating aggregated data such as total production and averages. Data aggregation is resource heavy task since each turbines produces data every 50 ms from 200 measurement points.

%\paragraph{The benefits}
By decentralizing the external communication interfaces, availability of the decentralized version of the current Siemens systems can be increased. Currently if the Wind Power Supervisor is unavailable external communication will be limited. In a decentralized solution a failing server can be discovered and a backup could replace it immediately. Furthermore if the number of external clients should increase it will be simpler to share workload across turbines.

%\paragraph{The problem}
A problem arises when external systems need to interact with the wind farm. 
Transforming external communication interfaces from being central to being part of the turbines creates a problem for client applications.
With multiple turbines able to serve requests and no guarantee that all turbines are online, how do they know which turbine to connect to?
Currently Siemens Wind Power uses a single ip address for external communication, it is preferred to also do so in an a decentralized version of the current Siemens system.
Another problem is that the turbines have limited processing power, it is preferred to have the added load of external communication divided evenly among the turbines of the wind farm.

\FloatBarrier
\subsection{Load Balancing}
A load balancer is a network component capable of distributing incoming connections from external clients to multiple server.

\begin{figure}
	\centering	
	%	\scalebox{0.7}{\input{figures/tikz/LoadBalancerOverview}}
	\scalebox{0.7}{\input{figures/tikz/ReguestRoutingInsteadOfBalancing}}
	\captionsetup{format=plain,font=footnotesize,labelfont={bf,defaultCapFont},labelsep=quad,singlelinecheck=no}
	\caption[Distributed system with two load balancing turbines]{
		\label{fig:loadBalancingSetup2Balancers} 
		\footnotesize{%
			A distributed system with two load balancing turbines.
		}
	}
\end{figure}


\subsubsection{Single entry point}
Providing a single entry point is as simple as assigning a static ip address to a turbine, however this introduces a single point of failure should the turbine become unavailable.
To overcome this the load balancer must be supplemented by at least one backup load balancer which, in the event of failure, can take over the address of the failing load balancer in order to keep the wind farm available.
This can be done using a virtual ip address, and having the turbine with the active load balancer respond to it.
If the turbine with the active load balancer becomes unavailable the virtual ip address can be transferred to a backup load balancer which will then become the active load balancer. To let other network participant know that the virtual ip address has moved to a new address a Address Resolution Request (ARP) announcement can be sent allowing them to update their ARP cache.

%\paragraph{Detecting node failure}
Detection of an unavailable load balancer must be done by the backup turbines, generally there are two ways this can be accomplished, either by actively pinging the active load balancer continuously or having the load balancer periodically send data package also know as heartbeats.
Besides monitoring the active load balancer the backup load balancers must also keep a synchronised copy of the active load balancers connections table that contains a list of open connections between clients and servers.
This setup is illustrated in \cref{fig:loadBalancingSetup2Balancers}.


\subsubsection{Load balancer forwarding}
The key objective of a load balancer is connecting clients to servers.
When a load balancer receives a package it must identify if the package belongs to a already open connection.
If the package belongs to a already open connection, the package is sent to the server which originally received the connection, if not a scheduling strategy is used to forward the package and the connection is recorded as open between the client and server in a table by the load balancer.
When a connection is closed or has timed out it is removed from the load balancers table. See the illustration to the left in \cref{fig:LoadbalancerOperation}. The grey dashed arrows are the connection as perceived by the client and the black arrows are the real connection handled by the load balancer.

There are different ways to forward packages but in general three are prominent:
\begin{description}
	\item[Network address translation (NAT)] is a basic version which functions like the NAT in home routers.
	The load balancer forwards the package to a server on the network, when the server responds it does the reverse and forwards the package to the client.
	This means all traffic must pass through the load balancer, this is seen in \cref{fig:LoadbalancerOperation} left illustration. 
	
	\item[Tunnelling] can be used to forward pacakges to the server, this is done by wrapping the package inside another package and then transmit it to the server.
	The server then unwraps the package and processes it. This requires the server to be setup to unwrap these packages.
	For this to work the servers must also be setup with the same virtual ip as the load balancer and be configured to not respond to ARP requests for that virtual ip address.
	When the server is done processing the unwrapped package it can now respond using the virtual ip address directly to the client avoiding the load balancer.
	This method has less overhead because of less communication. The principle is presented in \cref{fig:LoadbalancerOperation} right illustration.
	This method works across Ethernet segments.
	
	\item[Direct routing] is similar to tunnelling it forwards the package by changing the receivers mac address instead of wrapping the package.
	Again the server must be configured with the same virtual ip address to not respond to ARP packages for that virtual ip address, but in this setup it must also be connected to the same Ethernet segment as mac addresses are not used when routing across Ethernet segments.
	This method has the least overhead, but it does not work across Ethernet segments.
	Both tunnelling and forwarding using mac address are illustrated in \cref{fig:LoadbalancerOperation} to the right, the perceived connection is dashed and grey when different form the real flow in black.
	
\end{description}

\begin{figure}
	\centering
	\scalebox{0.7}{\input{figures/tikz/loadBalancerCommunication}}
	\scalebox{0.7}{\input{figures/tikz/loadBalancerCommunicationTCPHandoff}}
		\captionsetup{format=plain,font=footnotesize,labelfont={bf,defaultCapFont},labelsep=quad,singlelinecheck=no}
	\caption[Load balancer data transportation]{
		\label{fig:LoadbalancerOperation} 
		\footnotesize{%
			Load balancer data transportation. The left side illustrates load balancing with no direct server client communication and the right side illustrates a direct server to client communication.
		}
	}
\end{figure}


\subsubsection{Job scheduling strategies} %DNS Datacenter Application
There are multiple ways to distribute traffic that could apply to a decentralized solution.
%Load balancing can be done taking into account many parameters from a simply passing packets to servers in a round robin pattern, to monitoring performance parameters of the servers selecting the one with most free capacity. Some load balancers even inspect packages e.g. pass the HTTP headers URL field to select a server responsible for a specific resource.
Solutions that require client integration or is made for entirely different purposes are omitted as they are not usable for a system that has to be decentralized internally with a single entry point.
Job scheduling strategies are presented below:

\begin{description}
	\item[Round robin] as the name implies distributes connections to servers equally to each server, it is the most basic of the strategies discussed. In many situations this will give a fair distribution of workload, however this is only true if connecting clients require equal amount of data per connection. If the system has an asymmetric load round robin can provide an unfair distribution of workload. 
	Round robin has the least overhead.
	
	\item[Least connection] takes advantage of the fact that the load balancer need to keep track of open connections. This information is used in the strategy to assign new connections to the node which currently has the fewest connections.
	This ensures that long running connections are not causing nodes to be overloaded with new connections.
	This is done without adding extra overhead since data is already available at the load balancer.
	
	\item[Shortest expected delay] strategy keeps track of when a server has received a request and when it is expected to be finished processing a requests. The processing time for a single request is statically defined per server.
	% (Ci + 1) / Ui Ci equal number of server i connections, Ui equal the time the servers takes to process a single request.
	
	%\item[Performance monitoring] adds overhead because additional network packages has to be exchanged to keep track of the current performance of the servers. It however ensures that the server with the most free capacity will receive the connection. Free capacity can be defined as free processing time, available memory or any other resource.
	
	\item[Package inspection] opens up the network package and reads higher level protocol headers. HTTP headers could be read to identify what service the URL is trying to connect to. This can be handy when using a restful Application Programming Interface (API) to for instance build a monitoring platform.
	It also makes it possible to divide services up in slow and fast running services, to make sure that services which needs to respond fast are not being slowed down by services that perform data heavy calculations.
	Using package inspection would still allow other distribution strategies to further divide the workload.
	Furthermore it can be used to access session information and make sure that each user with the same session id gets redirected to the same server, even though using a different connection.
	Doing package inspection uses more processing time on the load balancer than the other options, as several layers of network abstractions needs to be processed to get to the relevant information. %HTTP and Modbus protocols are both part of the application layer and in Layer 7 in the OSI-model, this means that the load balancer need to process multiple layers of the network protocol to retrieve this information. 
	
\end{description}
None of the job scheduling strategies or load balancer forwarding described above scales infinitely as they all are based on a single active load balancer handling all incoming traffic. However, the methods described should be more than enough to handle the 100 concurrent connections that the current Siemens system currently handles. 

%\paragraph{Extra features}

%\paragraph{Existing systems}
\subsubsection{State of the art load balancers}
There exist a number of projects capable of performing load balancing as presented in the previous sections.


\begin{description}
\item[Linux Virtual Server Project~\cite{zhang2000linuxVirtualServer} (LVS)] is built into the Linux kernel.
Because LVS is build into the kernel it can redirect traffic on the data-link layer (OSI layer 2).
It supports load balancer failover based on heart beat monitoring.
LVS supports Round robin, Least-Connection and Shortest Expected Delay Scheduling job scheduling strategies.

\item[NGINX~\cite{NGINX_LoadBalancer}] is an application level load balancer with a built in high performance webserver, it is able to do http header inspection and use the information of the headers to distribute traffic to servers in groups.
The server support round robin, least connected, ip hash and hash load balancing strategies. 
NGINX in the basic open source version is unable to work directly with TCP/IP connections, a paid NGINX plus version exist which can do this.
TCP/IP connection support would be needed if other TCP/IP based protocols should be supported like for instance Modbus.
NGINX works on both Linux and windows platforms.

%http://www.cisco.com/c/en/us/support/docs/ip/border-gateway-protocol-bgp/5212-46.html
%\item[Cisco Routers] Load balancing is built into all of Cisco's routers which supports, 
\end{description}
%
LVS should be able to outperform NGINX in terms of performance since LVS supports letting the server communicate directly back to the client.
Since most communication in a monitoring system is from source to sink, in this situation from wind farm to client application, it is fair to assume that more packages are being sent to the client application than to the server residing in the wind farm.
NGINX comes with http header inspection capabilities, however the decentralized version of the current Simens system must support more than just http clients.
Both load balancers work directly with TCP/IP connections.
A solution could be created which uses NGINX on top of LVS to handle sessions, and fine grained distribution of requests based on URL.

LVS is the best choice for the decentralized version of the current Siemens system because of the performance gains obtained by the kernel level optimization.

%\begin{figure}
%	\centering
%	\scalebox{0.7}{\input{figures/tikz/SystemOverviewFromLoadBalancer}}
%	\captionsetup{format=plain,font=footnotesize,labelfont={bf,defaultCapFont},labelsep=quad,singlelinecheck=no}
%	\caption[System overview from the load balancer viewpoint]{
%		\label{fig:parkOverview} 
%		\footnotesize{%
%			A system overview of park nodes.
%		}
%	}
%\end{figure}



% % % % Below snippet migt be usefull for descriping docker % % % % % % %
%Resource management is a key activity in a distributed decentralised system.
%A decentralised system consists of a number of nodes with various resources available.
%Every node in a distributed version of the current Siemens system will need to handle data logging (disk I/O), replication (Network), setpoint calculation(Processing Time).
%Having a regulation cycle across a network means sharing states. When this regulation cycle is in the milliseconds range the amount of data that needs to be transferred increases.
%Besides what every node needs to handle there are other tasks required by the current Siemens system that can not or does not need to be run on all nodes. These are data aggregation(Processing, Memory) and external interfaces like HTTP, Modbus, etc (Processing, Network).
%The resources important when making a distributed version of the Siemens case are here by identified as primarily: Processing time and Network bandwidth, secondarily: Hard drive I/O and memory operations.
% % % % Above snippet migt be usefull for descriping docker % % % % % % %

%Resource management should be fair, this means the nodes should have the workload distributed as evenly as possible.
%There are different ways to handle this, some systems are able to split a single task up in equally large chunks for each node, in the Siemens case this could be the control algorithm that every turbine controller will need to run.
%In a system where the tasks can be split evenly among the nodes, resource management logic might not be needed as the system achieves a fair distribution by design, this can result in a much simpler design because of fewer components while also avoiding overhead due to resource management control algorithms.
%Some systems have many different tasks that runs independently of each other in the Siemens case this will be handling requests from secondary systems running outside the wind farm.
%The current Siemens system currently is designed to handle a hundred simultaneously connection from outside the wind farm. This workload needs to distributed fairly when building a decentralised system therefore dedicated resource management logic is needed.


% % % %%SingleEntryPoint and List:LB:LoadBalFailover % % % % % % % %
%The systems needs to respond to a single external IP address. To do this multiple nodes share the same IP address, multiple protocols exists which can do this "Common Address Redundancy Protocol\todo{https://www.freebsd.org/doc/handbook/carp.html}, Virtual Router Redundancy Protocol\todo{http://tools.ietf.org/html/rfc2338} and~\cite{zhang2000linuxVirtualServer}".\todo{Put in refferences instead} To provide seamless transfer of the virtual ip address ARP announcements are used to inform the local network of the transfer.
%The backup load balancer monitors the active load balancer and will takeover the virtual ip when the Active load balancer is unavailable and itself become primary this is illustrated in \cref{fig:loadBalancingSetup2Balancers}, this monitoring can be done with the active client sending out periodic heartbeats, or the backup sending ICMP ECHO\_REQUEST.
%Property \ref{List:LB:DetectFailedNodes} requires the load balancer to monitor local nodes for availability, this is to ensure that client connections are not being redirected to a faulty node. This is most easily achived with a ICMP ECHO\_REQUEST otherwise known as a ping request, 


 %\cite{Citation is missing and I need to find it}.
%There exists a lot of solution specific load balancers.
%A node balancer is a service which distributes incoming requests, among the services registered on the network.
%The distribution is based on different policies like dividing packages or picking the one with most free CPU capacity.
%The load balancer could be a single point of failure, it should for this reason always have one or more backups as seen in \cref{fig:loadBalancingSetup}.
%The load balancer monitors the real servers and only parses on requests to running servers.
%A secondary needs to monitor the primary load balancer and step in if it stops responding.
%This can be done using a virtual private IP, which is the address for all incoming requests. Also there exists solutions which does all this in a completely distributed manner avoiding having a central server as load balancer  \textbf{[?]}, %\cite{Some cite I ahven't found yet}, 

%In this solution the load balancer needs to balance external connections to different protocols like HTTP and Modbus, however a solution which can be extended to any restful protocol is needed.
%Also balancing of node roles depending on the amount incoming traffic on different interfaces will be needed.
%Should all this be done automatically or is it better to define this manually???

%Load balances can provide various features, related to offloading the real server and node management. % SSL decryprtion, caching, authentication, ...

 
%\paragraph{consider the} 
%\begin{itemize}
%\item job migration cost
%\item resource heterogeneity
%\item network heterogeneity
%
%\item resource usage
%\item conditions
%\end{itemize}
%
%\paragraph{performance parameters}
%\begin{itemize}
%\item job size
%\item data transfer rate
%\item status exchange period
%\item migration limit
%\end{itemize}

%\paragraph{Algorithms/policies}
%\begin{itemize}
%	%(On the Design of Adaptive and Decentralized	Load-Balancing Algorithms with Load Estimation for Computational Grid Environments)	
%	\item MELISA Modified ELISA (used for large scale system (interGrid))
%	\item LBA Load Balancing on Arrival
%	
%	\item Round-robin (Weighted/unweighed)
%	\item Least-Connection (Weighted/unweighed)
%\end{itemize}
%
%\paragraph{The following requirements to the system exists}
%\begin{itemize}
%	\item Robustness
%	\item Protocol flexible
%	\item Must be a distributed component
%\end{itemize}
%
%\paragraph{Preferred features}
%\begin{itemize}
%	\item support TCP Handoffs (for non restful applications)
%\end{itemize}
%
%\subsection{Levels of balancing}
%\begin{description}
%	\item[??] DDNS / Round robin DNS
%	\item[OSI 3] Network/IP %google says network layer LVS says transport layer
%	\item[OSI 4] Network/IP
%	\item[OSI 7] {Application level, like http balancing, allows balancing strategies based on url and user location.}
%\end{description}
%
%What we would like is a transport layer protocol.
%\vspace{1cm}

%\noindent\cite{Ludwig:SwarmIntelligenceGridLoadBalancing} 
%Implements a particle swam based and an ant based algorithm. 
%It discuses quality parameters.
%It compares with a version using distributed knowledge (SBA State Broadcast Algorithm)
%\\

%\noindent\cite{MayuriMehta:HybridDynamicLB} Central and distributed Balancing (hybrid). Central(3) dosn't scale, distributed (6 refs) has a larger overhead. Implements node selection algorithms.
%
%Nice comparison of load balancers
%\url{http://codeghar.wordpress.com/2007/11/04/create-a-load-balance-server-using-ubuntu/}
%
%hearbeat 	\url{http://people.adams.edu/~cdmiller/posts/nginx-heartbeat-ha/}
%
%\url{http://kaivanov.blogspot.dk/2013/01/building-load-balancer-with-lvs-linux.html}
%\url{http://sorrawut.com/lvs-dr/}
%\subsection{Existing solutions}
%\begin{description}
%	\item [Linux Virtual Server: IPVS] Is implemented in the linux kernal version 2.4 and 2.6. Works at the IP level. Used by big sites sourceforge.net, layer 3.
%	\item [Google Compute Engine: Load Balancer]: Proprietary. layer 3 and 7.
%	%http://aws.amazon.com/route53/
%	\item [Amazon Route 53] DNS based server, provides region based balancing and latency balancing.
%	\item [AWS ELB] Amazons load balancer, Proxy based, top tier
%	%https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance
%	\item [Eureka] Netflix Load Balancer, client based, middle tier
%
%	\item [Loadbalancer.org] ...
%	\item [HAProxy] ...
%	\item [pfsense] ...
%	\item [http://www.ultramonkey.org/]
%	\item [Balance] ...
%	\item [Crossroads] ...
%	\item [nginx] \url{http://nginx.org/en/docs/http/load_balancing.html}
%	\item [varnish-cache] \url{https://www.varnish-cache.org/trac/wiki}
%\end{description}

%\begin{itemize}
%	\item batch schedulers
%	\item workflow engines
%	\item operating systems (build in scheduler)
%\end{itemize}

%\begin{figure}
%	\centering	
%	\scalebox{0.7}{\input{figures/tikz/ReguestRoutingInsteadOfBalancing}}
%	\captionsetup{format=plain,font=footnotesize,labelfont={bf,defaultCapFont},labelsep=quad,singlelinecheck=no}
%	\caption[Routing requests as load balancing]{
%		\label{fig:RoutingloadBalancingSetup} 
%		\footnotesize{%
%			Routing requests as load balancing.
%		}
%	}
%\end{figure}


% % % SSK Might Be usable later until then hidden here % % %
% % % DNS load balancers are not relevant to the Simens case. % % % %
%Different load balancing methods will are here briefly described:
%%Load balancing for large systems can be split in tiers.
%\paragraph{Dynamic DNS}
%is a load balancer built into the DNS service, it distributes load in a round robin manner as well as using region knowledge of where the request was revived from~\cite{Amazon:Route53}. DNS based load balancing is among the most scalable solutions that does not need any special configuration of either client nor server. However is does not provide any monitoring of server performance parameters, and once the DNS lookup is over the service does not play any role in load balancing related to one that specific client until the DNS cache is refreshed. This type of loadbalancing is not applicable since 




\FloatBarrier







